{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification on Amazon Fine food Dataset from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. \n",
    "## Task\n",
    " \n",
    "#### Input - ProductId, UserId, ProfileName, HelpfulnessNumerator,HelpfulnessDenominator,Time,Summary,Text \n",
    "#### Ouput- To determine polarity if a review is positive/negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data=pd.read_sql_query(\"\"\"select * from Reviews where score!=3 \"\"\",con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We classify the score(4-5) as positive and score(1-2) as negative and discard score=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525809</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525810</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525811</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525812</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525813</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525814 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "525809  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "525810  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "525811  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "525812  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "525813  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "525809                     0                       0      5  1299628800   \n",
       "525810                     0                       0      2  1331251200   \n",
       "525811                     2                       2      5  1329782400   \n",
       "525812                     1                       1      5  1331596800   \n",
       "525813                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "525809                 Will not do without   \n",
       "525810                        disappointed   \n",
       "525811            Perfect for our maltipoo   \n",
       "525812  Favorite Training and reward treat   \n",
       "525813                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "525809  Great for sesame chicken..this is a good if no...  \n",
       "525810  I'm disappointed with the flavor. The chocolat...  \n",
       "525811  These stars are small, so you can give 10-15 o...  \n",
       "525812  These are the BEST treats for training and rew...  \n",
       "525813  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[525814 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_score=filtered_data['Score']\n",
    "positivenegative=real_score.map(partition)\n",
    "filtered_data['Score']=positivenegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df=filtered_data.sort_values('ProductId',axis=0,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates for same user with text for different products on same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df=sorted_df.drop_duplicates(subset={'UserId','ProfileName','Time','Text'},keep='first',inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data remaining after removing duplicates = 69% (intial = 525814, now=364173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=sorted_df[sorted_df.HelpfulnessNumerator <= sorted_df.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpfulness numerator is total positive reviews and helpfullness denominator is total of both +ve,-ve. so, removing redudunant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to vector mapping\n",
    "### 1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer()\n",
    "# It will convert all text to lower case by default\n",
    "final_counts=count_vec.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### final_counts is a sparse matrix represenattion\n",
    "final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-preprocessing - Stopwords,lemmatization,stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removing HTML tags\n",
    "2. Removing punctuations\n",
    "3. Check for english letter and word is not alpha-numeric\n",
    "4. len(words)>2\n",
    "5. converting words to lowercase \n",
    "6. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "snow=nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(sent):\n",
    "    clean=re.compile('<.*?>')\n",
    "    cleantext=re.sub(clean,' ',sent)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpunc(sent):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#]',r'',sent)\n",
    "    cleaned=re.sub(r'[.|,|)|(|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "final_string=[]\n",
    "pos_words=[]\n",
    "neg_words=[]\n",
    "for sent in final['Text'].values:\n",
    "    fil_sen=[]\n",
    "    sent=clean_html(sent)\n",
    "    for w in sent.split():\n",
    "        for clean_words in cleanpunc(w).split():\n",
    "            if((clean_words.isalpha())&(len(clean_words)>2)):\n",
    "                if(clean_words.lower() not in stop):\n",
    "                    s=(snow.stem(clean_words.lower())).encode('utf8')\n",
    "                    fil_sen.append(s)\n",
    "                    if((final['Score'].values)[i]=='positive'):\n",
    "                        pos_words.append(s)\n",
    "                    else:\n",
    "                        neg_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    strl=b\" \".join(fil_sen)\n",
    "    final_string.append(strl)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vingo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final['cleaned_text']=final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "      <td>b'grew read sendak book watch realli rosi movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "      <td>b'fun way children learn month year learn poem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "      <td>b'great littl book read nice rhythm well good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "      <td>b'book poetri month year goe month cute littl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text  \\\n",
       "138706  this witty little book makes my son laugh at l...   \n",
       "138688  I grew up reading these Sendak books, and watc...   \n",
       "138689  This is a fun way for children to learn their ...   \n",
       "138690  This is a great little book to read aloud- it ...   \n",
       "138691  This is a book of poetry about the months of t...   \n",
       "\n",
       "                                             cleaned_text  \n",
       "138706  b'witti littl book make son laugh loud recit c...  \n",
       "138688  b'grew read sendak book watch realli rosi movi...  \n",
       "138689  b'fun way children learn month year learn poem...  \n",
       "138690  b'great littl book read nice rhythm well good ...  \n",
       "138691  b'book poetri month year goe month cute littl ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[['Text','cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Grams and N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating freq of +ve and -ve words\n",
    "freq_dist_pos=nltk.FreqDist(pos_words)\n",
    "freq_dist_neg=nltk.FreqDist(neg_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common +ve words [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n",
      "Most common -ve words [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common +ve words',freq_dist_pos.most_common(20))\n",
    "print('Most common -ve words',freq_dist_neg.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering bigram,trigram,ngram because of overlap of words frokm positive and negative freq dist eg. like occured both in +ve and -ve reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(ngram_range=(1,2))\n",
    "final_counts=count_vec.fit_transform(final['cleaned_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2923725)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After calculating bigrams dimensions increased from (364171, 115281) to (364171, 2923725)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_vec.fit_transform(final['cleaned_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2923725)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=tf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acess keurig',\n",
       " 'acesufam',\n",
       " 'acesufam potassium',\n",
       " 'acesuflamek',\n",
       " 'acesuflamek sacharin',\n",
       " 'acesulf',\n",
       " 'acesulf also',\n",
       " 'acesulf found',\n",
       " 'acesulfam']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[10000:10009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(rows,features,top_n=25):\n",
    "    top_ids=np.argsort(rows)[::-1][:top_n]\n",
    "    top_feat=[(features[i],rows[i]) for i in top_ids]\n",
    "    df=pd.DataFrame(top_feat)\n",
    "    df.columns=['features','tfidf']\n",
    "    return df\n",
    "top_idf=top_tfidf_features(final_tf_idf[1,:].toarray()[0],features,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sendak books</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosie movie</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paperbacks seem</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these sendak</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the paperbacks</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pages open</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really rosie</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>incorporates them</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paperbacks</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>however miss</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.161317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up reading</td>\n",
       "      <td>0.156867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that incorporates</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the pages</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rosie</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>of flimsy</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two hands</td>\n",
       "      <td>0.145130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>movie that</td>\n",
       "      <td>0.144374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reading these</td>\n",
       "      <td>0.137184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>too do</td>\n",
       "      <td>0.134491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>incorporates</td>\n",
       "      <td>0.134147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flimsy and</td>\n",
       "      <td>0.132254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             features     tfidf\n",
       "0        sendak books  0.173437\n",
       "1         rosie movie  0.173437\n",
       "2     paperbacks seem  0.173437\n",
       "3       cover version  0.173437\n",
       "4        these sendak  0.173437\n",
       "5      the paperbacks  0.173437\n",
       "6          pages open  0.173437\n",
       "7        really rosie  0.168074\n",
       "8   incorporates them  0.168074\n",
       "9          paperbacks  0.168074\n",
       "10       however miss  0.164269\n",
       "11         hard cover  0.164269\n",
       "12          seem kind  0.161317\n",
       "13         up reading  0.156867\n",
       "14  that incorporates  0.155100\n",
       "15          the pages  0.149737\n",
       "16             sendak  0.149737\n",
       "17              rosie  0.146786\n",
       "18          of flimsy  0.146786\n",
       "19          two hands  0.145130\n",
       "20         movie that  0.144374\n",
       "21      reading these  0.137184\n",
       "22             too do  0.134491\n",
       "23       incorporates  0.134147\n",
       "24         flimsy and  0.132254"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "i=0\n",
    "list_sent=[]\n",
    "for sent in final['Text'].values:\n",
    "    fil_sen=[]\n",
    "    sent=clean_html(sent)\n",
    "    for w in sent.split():\n",
    "        for clean_words in cleanpunc(w).split():\n",
    "            if((clean_words.isalpha())):\n",
    "                fil_sen.append(clean_words)\n",
    "            else:\n",
    "                continue\n",
    "    list_sent.append(fil_sen)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, India, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will STILL be able to recite from memory when he is  in college\n",
      "******************************************************\n",
      "['this', 'witty', 'little', 'book', 'makes', 'my', 'son', 'laugh', 'at', 'loud', 'i', 'recite', 'it', 'in', 'the', 'car', 'as', 'were', 'driving', 'along', 'and', 'he', 'always', 'can', 'sing', 'the', 'refrain', 'hes', 'learned', 'about', 'whales', 'India', 'drooping', 'i', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'of', 'it', 'all', 'this', 'is', 'a', 'classic', 'book', 'i', 'am', 'willing', 'to', 'bet', 'my', 'son', 'will', 'STILL', 'be', 'able', 'to', 'recite', 'from', 'memory', 'when', 'he', 'is', 'in', 'college']\n"
     ]
    }
   ],
   "source": [
    "print(final['Text'].values[0])\n",
    "print(\"******************************************************\")\n",
    "print(list_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_count indicates words with min 5 occurances and construct a vector\n",
    "# size - dimensions of vectors to be constructed.(more corpus, size should be high)\n",
    "# workers - for using cores in cpu\n",
    "w2v_model=gensim.models.Word2Vec(list_sent,min_count=5,size=50,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44605\n"
     ]
    }
   ],
   "source": [
    "words=list(w2v_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastey', 0.9135609865188599),\n",
       " ('yummy', 0.8866933584213257),\n",
       " ('satisfying', 0.8614116907119751),\n",
       " ('delicious', 0.8450340032577515),\n",
       " ('filling', 0.8422430753707886),\n",
       " ('flavorful', 0.8268660306930542),\n",
       " ('addicting', 0.797648549079895),\n",
       " ('versatile', 0.7878106236457825),\n",
       " ('nutritious', 0.770317554473877),\n",
       " ('delish', 0.7684949636459351)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part(x):\n",
    "    if x==\"positive\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_tf_idf\n",
    "y_map=final['Score'].map(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vingo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "final['score_n']=y_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(final['score_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp_pos=final[final['score_n']==1].sample(n=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp_neg=final[final['score_n']==0].sample(n=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp=pd.concat([df_samp_pos,df_samp_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting data based on time\n",
    "df_samp['Time'] = pd.to_datetime(df_samp['Time'], unit = 's')\n",
    "df_samp = df_samp.sort_values(by = 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>score_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346041</th>\n",
       "      <td>374343</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>negative</td>\n",
       "      <td>2000-01-19</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "      <td>b'alway enjoy movi funni entertain didnt hesit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346054</th>\n",
       "      <td>374358</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A1HWMNSQF14MP8</td>\n",
       "      <td>will@socialaw.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>2000-12-30</td>\n",
       "      <td>A Afterlife Success</td>\n",
       "      <td>Many movies, have dealt with the figure of dea...</td>\n",
       "      <td>b'mani movi dealt figur death dead good angel ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346077</th>\n",
       "      <td>374382</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A3C3BAQDZWH5YE</td>\n",
       "      <td>Kushana no shinryaku (Kushana's invasion)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>2002-02-26</td>\n",
       "      <td>...</td>\n",
       "      <td>It was on the other night, and, having been a ...</td>\n",
       "      <td>b'night big fan cartoon shown decid watch also...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121056</th>\n",
       "      <td>131233</td>\n",
       "      <td>B00004RAMX</td>\n",
       "      <td>A1PYZPS1QYR036</td>\n",
       "      <td>Kazantzakis \"hinterlands\"</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>negative</td>\n",
       "      <td>2003-10-29</td>\n",
       "      <td>Woodstream Gopher Trap 0610</td>\n",
       "      <td>This is a poor excuse for a gopher trap. I hav...</td>\n",
       "      <td>b'poor excus gopher trap lot gopher use trap r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264269</th>\n",
       "      <td>286453</td>\n",
       "      <td>B0000DJ7WI</td>\n",
       "      <td>A375QRG43POEW6</td>\n",
       "      <td>dndnd</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>negative</td>\n",
       "      <td>2003-11-28</td>\n",
       "      <td>Stay Away...</td>\n",
       "      <td>This product ships in great packaging and it l...</td>\n",
       "      <td>b'product ship great packag look good tast hor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId  \\\n",
       "346041  374343  B00004CI84   A1B2IZU1JLZA6   \n",
       "346054  374358  B00004CI84  A1HWMNSQF14MP8   \n",
       "346077  374382  B00004CI84  A3C3BAQDZWH5YE   \n",
       "121056  131233  B00004RAMX  A1PYZPS1QYR036   \n",
       "264269  286453  B0000DJ7WI  A375QRG43POEW6   \n",
       "\n",
       "                                      ProfileName  HelpfulnessNumerator  \\\n",
       "346041                                        Wes                    19   \n",
       "346054                          will@socialaw.com                     1   \n",
       "346077  Kushana no shinryaku (Kushana's invasion)                     0   \n",
       "121056                  Kazantzakis \"hinterlands\"                     5   \n",
       "264269                                      dndnd                    23   \n",
       "\n",
       "        HelpfulnessDenominator     Score       Time  \\\n",
       "346041                      23  negative 2000-01-19   \n",
       "346054                       2  positive 2000-12-30   \n",
       "346077                       1  positive 2002-02-26   \n",
       "121056                       8  negative 2003-10-29   \n",
       "264269                      25  negative 2003-11-28   \n",
       "\n",
       "                                                Summary  \\\n",
       "346041  WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "346054                              A Afterlife Success   \n",
       "346077                                              ...   \n",
       "121056                      Woodstream Gopher Trap 0610   \n",
       "264269                                     Stay Away...   \n",
       "\n",
       "                                                     Text  \\\n",
       "346041  I, myself always enjoyed this movie, it's very...   \n",
       "346054  Many movies, have dealt with the figure of dea...   \n",
       "346077  It was on the other night, and, having been a ...   \n",
       "121056  This is a poor excuse for a gopher trap. I hav...   \n",
       "264269  This product ships in great packaging and it l...   \n",
       "\n",
       "                                             cleaned_text  score_n  \n",
       "346041  b'alway enjoy movi funni entertain didnt hesit...        0  \n",
       "346054  b'mani movi dealt figur death dead good angel ...        1  \n",
       "346077  b'night big fan cartoon shown decid watch also...        1  \n",
       "121056  b'poor excus gopher trap lot gopher use trap r...        0  \n",
       "264269  b'product ship great packag look good tast hor...        0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 12)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dealing with timestamp, we take first 8400 timestamps as train and remaining as test data instead of random split because reviews change according to time and random split performance is less than training into certain intervals of time and predicting next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasplit_train=df_samp.iloc[:8400,:]\n",
    "datasplit_test=df_samp.iloc[8400:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400, 12)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasplit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 12)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasplit_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Applying BOW vectorization on text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer()\n",
    "# It will convert all text to lower case by default\n",
    "final_counts=count_vec.fit_transform(datasplit_train['cleaned_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array(datasplit_train['score_n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't perform fit_transform for text data as count_vect is already trained use transform and use count_vect from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# It will convert all text to lower case by default\n",
    "final_counts=count_vec.transform(datasplit_test['cleaned_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.array(datasplit_test['score_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further dividing data into training and cross-validation sets\n",
    "X_tr, X_cv, y_tr, y_cv = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n_estimators 120\n",
      "optimal max_depth 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "scalar = StandardScaler(with_mean=False)\n",
    "X_tr= scalar.fit_transform(X_tr)\n",
    "X_test= scalar.transform(X_test)\n",
    "X_cv=scalar.transform(X_cv)\n",
    "\n",
    "base_learners = [20,40,60,80,100,120]\n",
    "depths=[1,5,10,50,100,500,1000]\n",
    "param_grid={'n_estimators': base_learners, 'max_depth':depths}\n",
    "rf = RandomForestClassifier(max_features='sqrt')\n",
    "model=GridSearchCV(rf,param_grid,scoring='roc_auc',n_jobs=-1,cv=3)\n",
    "model.fit(X_tr,y_tr)\n",
    "print(\"optimal n_estimators\",model.best_estimator_.n_estimators)\n",
    "print(\"optimal max_depth\",model.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "1. optimal base models (n_estimators) :  120\n",
    "2. optimal max tree Depth  :  100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(max_features='sqrt',max_depth=100,n_estimators=120)\n",
    "rf.fit(X_tr,y_tr)\n",
    "pred=rf.predict(X_test)\n",
    "acc=accuracy_score(Y_test,pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.83333333333334"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total accuracy = 82.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision=precision_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045454545454546"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision=80.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "rec_score=recall_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8378698224852071"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall=83.78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c_mat=confusion_matrix(Y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1566,  344],\n",
       "       [ 274, 1416]], dtype=int64)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying TFIDF vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_vec.fit_transform(datasplit_train['cleaned_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array(datasplit_train['score_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_counts=tf_vec.transform(datasplit_test['cleaned_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=np.array(datasplit_test['score_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_cv, y_tr, y_cv = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n_estimators 100\n",
      "optimal max_depth 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "scalar = StandardScaler(with_mean=False)\n",
    "X_tr= scalar.fit_transform(X_tr)\n",
    "X_test= scalar.transform(X_test)\n",
    "X_cv=scalar.transform(X_cv)\n",
    "\n",
    "base_learners = [20,40,60,80,100,120]\n",
    "depths=[1,5,10,50,100,500,1000]\n",
    "param_grid={'n_estimators': base_learners, 'max_depth':depths}\n",
    "rf = RandomForestClassifier(max_features='sqrt')\n",
    "model=GridSearchCV(rf,param_grid,scoring='roc_auc',n_jobs=-1,cv=3)\n",
    "model.fit(X_tr,y_tr)\n",
    "print(\"optimal n_estimators\",model.best_estimator_.n_estimators)\n",
    "print(\"optimal max_depth\",model.best_estimator_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "1. optimal base models (n_estimators) :  100\n",
    "2. optimal max tree Depth  :  100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(max_features='sqrt',max_depth=100,n_estimators=100)\n",
    "rf.fit(X_tr,y_tr)\n",
    "pred=rf.predict(X_test)\n",
    "acc=accuracy_score(Y_test,pred)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.86111111111111"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total accuracy = 82.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision=precision_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8043108338060124"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision=80.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "rec_score=recall_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8390532544378698"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall=83.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
